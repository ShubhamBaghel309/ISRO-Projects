import numpy as np
from sklearn.linear_model import Ridge
from scipy import sparse
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import pickle
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, loguniform
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from scipy import signal as sg

# Load the dataset using our helper function 
def load_gps_dataset(base_filename='enhanced_gps_dataset'):
    """
    Load the GPS dataset generated by gps_signal_dataset2.py
    
    Parameters:
    -----------
    base_filename: str
        Base filename used when saving the dataset
        
    Returns:
    --------
    X: numpy array
        Input features (jammed signal windows)
    y: numpy array
        Target outputs (clean signal windows)
    metadata: pandas DataFrame
        Metadata about each window including jammer type and JNR
    """
    X = np.load(f"{base_filename}_X_jammed.npy")
    y = np.load(f"{base_filename}_y_clean.npy")
    metadata = pd.read_csv(f"{base_filename}_metadata.csv")
    
    print(f"Loaded dataset with {X.shape[0]} windows")
    print(f"X shape: {X.shape}, y shape: {y.shape}")
    print(f"Jammer types: {metadata['jammer_type'].unique()}")
    print(f"JNR range: {metadata['jnr_db'].min():.1f}dB to {metadata['jnr_db'].max():.1f}dB")
    
    return X, y, metadata

# Load the dataset
print("Loading GPS signal dataset...")
X_all, y_all, metadata = load_gps_dataset()

# Enhanced preprocessing tailored specifically for GPS anti-jamming
def preprocess_signal(signal, fs=1000, jammer_type=None):
    """GPS-specific signal preprocessing with jammer type awareness"""
    # Apply notch filters for known jammers if jammer type is provided
    if jammer_type == "continuous":
        # Notch filter for continuous wave jammers (multiple possible frequencies)
        for freq in [0.05, 0.1, 0.15]:  # Normalized frequencies for simulation
            Q = 30.0  # Quality factor
            w0 = freq/(fs/2)
            b, a = sg.iirnotch(w0, Q)
            signal = sg.filtfilt(b, a, signal)
            
    elif jammer_type == "swept":
        # Wider band reject filter for swept frequency jammers
        b, a = sg.butter(8, [0.05, 0.15], 'bandstop')
        signal = sg.filtfilt(b, a, signal)
    
    # Apply a bandpass filter to isolate GPS L1 band frequencies
    b, a = sg.butter(6, [0.01, 0.2], 'bandpass')
    filtered = sg.filtfilt(b, a, signal)
    
    # Use wavelet denoising if available, otherwise alternative approach
    denoised = filtered
    try:
        import pywt
        wavelet = 'sym8'  # Better wavelet for GPS signals
        # Reduced level from 4 to 2 to avoid boundary effect warnings
        level = 2  
        coeffs = pywt.wavedec(filtered, wavelet, level=level)
        # Adaptive thresholding for each level
        for i in range(1, len(coeffs)):
            threshold = np.std(coeffs[i]) * np.sqrt(2*np.log(len(filtered)))
            coeffs[i] = pywt.threshold(coeffs[i], threshold, mode='soft')
        denoised = pywt.waverec(coeffs, wavelet)[:len(filtered)]
    except ImportError:
        # Enhanced alternative denoising approach
        # 1. First median filter to remove spikes
        denoised = sg.medfilt(filtered, kernel_size=5)
        # 2. Savitzky-Golay filter for smoothing while preserving peaks
        denoised = sg.savgol_filter(denoised, window_length=11, polyorder=3)
    
    # Calculate signal envelope for feature enhancement
    analytic_signal = sg.hilbert(denoised)
    envelope = np.abs(analytic_signal)
    phase = np.unwrap(np.angle(analytic_signal))
    inst_freq = np.diff(phase) / (2.0*np.pi) * fs
    
    # Create enhanced feature matrix including:
    # 1. Denoised signal
    # 2. Instantaneous frequency (helps detect frequency shifts in jammers)
    # 3. Signal envelope (helps with amplitude modulation detection)
    # 4. First derivative (helps detect abrupt changes)
    # 5. Second derivative (helps detect peaks)
    
    # Pad derivatives to match original length
    inst_freq_padded = np.pad(inst_freq, (1, 0), 'edge')
    first_derivative = np.gradient(denoised)
    second_derivative = np.gradient(first_derivative)
    
    # Create feature matrix
    features = np.column_stack([
        denoised,                    # Clean signal
        inst_freq_padded,            # Frequency information
        envelope,                    # Amplitude information
        first_derivative,            # Rate of change
        second_derivative,           # Acceleration (peak detection)
        sg.medfilt(denoised, 7)      # Additional smoothing
    ])
    
    # Standardize features for better network performance
    scaler = MinMaxScaler(feature_range=(-1, 1))
    features = scaler.fit_transform(features)
    
    return features

# GPS-specific reservoir architecture based on research papers
def create_gps_reservoir(input_signal, n_reservoir=800, sparsity=0.05, 
                        spectral_radius=0.95, noise=0.0001, leaking_rate=0.2,
                        input_scaling=1.2, seed=None):
    """
    Create a reservoir specifically designed for GPS signal processing
    with specialized neurons for different jamming patterns.
    """
    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)
    
    # Convert to 2D array if needed
    if len(input_signal.shape) == 1:
        input_signal = input_signal.reshape(-1, 1)
    
    # Number of input features
    n_inputs = input_signal.shape[1]
    
    # Create specialized neuron groups within the reservoir
    # 1. Standard neurons (70% of reservoir)
    n_standard = int(n_reservoir * 0.7)
    # 2. High-frequency neurons (15% of reservoir) - for capturing jamming patterns
    n_hf = int(n_reservoir * 0.15)
    # 3. Memory neurons (15% of reservoir) - for capturing temporal patterns
    n_memory = n_reservoir - n_standard - n_hf
    
    # Generate reservoir connections with structured sparsity
    # Use CSR matrix format which supports item assignment (unlike COO format)
    W = sparse.random(n_reservoir, n_reservoir, density=sparsity, format='csr')
    
    # Create denser connections within specialized neuron groups
    # This helps create specialized processing units
    for i in range(n_standard, n_standard + n_hf):
        for j in range(n_standard, n_standard + n_hf):
            if np.random.rand() < 0.3:  # 30% connection probability within group
                W[i, j] = np.random.randn() * 0.1
                
    for i in range(n_standard + n_hf, n_reservoir):
        for j in range(n_standard + n_hf, n_reservoir):
            if np.random.rand() < 0.4:  # 40% connection probability for memory neurons
                W[i, j] = np.random.randn() * 0.1
    
    # Scale by spectral radius
    eigenvalues = sparse.linalg.eigs(W, k=1, which='LM', return_eigenvectors=False)
    scaling_factor = spectral_radius / np.abs(eigenvalues[0])
    W = W * scaling_factor  # Multiply instead of using multiply() method
    
    # Create structured input weights
    W_in = np.zeros((n_reservoir, n_inputs))
    
    # Standard neurons receive all inputs
    W_in[:n_standard, :] = np.random.randn(n_standard, n_inputs) * input_scaling
    
    # High-frequency neurons receive stronger connections from frequency and derivative features
    freq_features = [1, 3, 4]  # Indices of frequency-related features
    W_in[n_standard:n_standard+n_hf, freq_features] = np.random.randn(n_hf, len(freq_features)) * (input_scaling * 1.5)
    
    # Memory neurons receive stronger connections from envelope and raw signal
    memory_features = [0, 2]  # Indices of amplitude-related features
    W_in[n_standard+n_hf:, memory_features] = np.random.randn(n_memory, len(memory_features)) * (input_scaling * 1.2)
    
    # Initialize reservoir state
    x = np.zeros(n_reservoir)
    
    # Initialize states collection
    states = np.zeros((len(input_signal), n_reservoir))
    
    # Run the reservoir with specialized dynamics
    for i in range(len(input_signal)):
        # Get input
        u = input_signal[i]
        
        # Standard neurons update
        x_new = np.tanh(np.dot(W_in, u) + W.dot(x) + noise * np.random.randn(n_reservoir))
        
        # Apply different leaking rates to different neuron groups
        # Standard neurons - normal leaking rate
        x[:n_standard] = (1 - leaking_rate) * x[:n_standard] + leaking_rate * x_new[:n_standard]
        
        # High-frequency neurons - faster leaking rate for quicker response to jamming
        x[n_standard:n_standard+n_hf] = (1 - leaking_rate*1.5) * x[n_standard:n_standard+n_hf] + \
                                       leaking_rate*1.5 * x_new[n_standard:n_standard+n_hf]
        
        # Memory neurons - slower leaking rate for longer memory
        x[n_standard+n_hf:] = (1 - leaking_rate*0.5) * x[n_standard+n_hf:] + \
                             leaking_rate*0.5 * x_new[n_standard+n_hf:]
        
        # Store state
        states[i] = x
        
    return states

# Calculate Bit Error Rate for GPS signals
def calculate_ber(y_true, y_pred, threshold=0.0):
    """
    Calculate Bit Error Rate for GPS signal recovery
    
    Parameters:
    -----------
    y_true: numpy array
        True clean signal
    y_pred: numpy array
        Predicted clean signal
    threshold: float
        Threshold for bit decision
        
    Returns:
    --------
    ber: float
        Bit error rate (0.0-1.0)
    """
    # Convert signals to bits using zero-crossing method (typical for GPS signals)
    y_true_bits = (y_true > threshold).astype(int)
    y_pred_bits = (y_pred > threshold).astype(int)
    
    # Calculate bit error rate
    bit_errors = np.sum(y_true_bits != y_pred_bits)
    total_bits = len(y_true_bits)
    
    return bit_errors / total_bits

# Enhanced evaluation function with BER
def evaluate_model(y_true, y_pred):
    """Evaluate model performance with multiple metrics including BER"""
    # Calculate overall metrics
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true.flatten(), y_pred.flatten())
    
    # Calculate SNR improvement
    input_power = np.mean(y_true**2)
    noise_power = np.mean((y_true - y_pred)**2)
    snr_db = 10 * np.log10(input_power / noise_power) if noise_power > 0 else float('inf')
    
    # Calculate BER (key metric from research paper)
    ber = calculate_ber(y_true.flatten(), y_pred.flatten())
    
    # Create report
    report = {
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'snr_db': snr_db,
        'ber': ber
    }
    
    return report

# Optimize reservoir for GPS signal recovery
def optimize_reservoir(X_train, y_train, X_val, y_val, metadata_train=None, metadata_val=None, n_iter=15):
    """Optimize reservoir computing hyperparameters with BER focus"""
    print("Optimizing reservoir for minimal BER...")
    
    # If metadata is not provided, use empty DataFrames
    if metadata_train is None:
        metadata_train = pd.DataFrame({'jammer_type': ['unknown'] * len(X_train)})
    if metadata_val is None:
        metadata_val = pd.DataFrame({'jammer_type': ['unknown'] * len(X_val)})
    
    best_ber = float('inf')
    best_params = {}
    
    # Convert inputs to numpy arrays if they aren't already
    X_train = np.array(X_train)
    y_train = np.array(y_train)
    X_val = np.array(X_val)
    y_val = np.array(y_val)
    
    # Parameter ranges optimized for GPS signal recovery based on research papers
    param_distributions = {
        'n_reservoir': [600, 800, 1000, 1200],      # Larger reservoirs (paper recommends 800+)
        'spectral_radius': uniform(0.85, 0.14),      # High spectral radius (0.85-0.99)
        'sparsity': uniform(0.01, 0.09),            # More sparse (0.01-0.1)
        'noise': loguniform(1e-6, 1e-3),            # Low noise
        'leaking_rate': uniform(0.1, 0.4),          # Research shows 0.1-0.5 works well
        'input_scaling': uniform(0.8, 1.6)          # Higher input scaling for jammed signals
    }
    
    results = []
    
    for i in tqdm(range(n_iter), desc="Hyperparameter optimization"):
        # Sample parameters
        params = {
            'n_reservoir': int(np.random.choice(param_distributions['n_reservoir'])),
            'spectral_radius': float(param_distributions['spectral_radius'].rvs()),
            'sparsity': float(param_distributions['sparsity'].rvs()),
            'noise': float(param_distributions['noise'].rvs()),
            'leaking_rate': float(param_distributions['leaking_rate'].rvs()),
            'input_scaling': float(param_distributions['input_scaling'].rvs()),
            'seed': 42 + i  # Different seed for each iteration but reproducible
        }
        
        # Process a subset of training windows
        subset_size = min(100, len(X_train))
        train_subset_idx = np.random.choice(len(X_train), subset_size, replace=False)
        X_train_subset = X_train[train_subset_idx]
        y_train_subset = y_train[train_subset_idx]
        
        # Create states for training subset
        train_states = []
        for window in X_train_subset:
            state = create_gps_reservoir(window, **params)
            train_states.append(state)
        
        # Flatten for training
        train_states = np.array(train_states)
        X_train_flat = train_states.reshape(-1, train_states.shape[-1])
        y_train_flat = y_train_subset.reshape(-1)
        
        # Train ridge regression
        readout = Ridge(alpha=0.1)
        readout.fit(X_train_flat, y_train_flat)
        
        # Evaluate on validation data
        val_preds = []
        for window in X_val[:50]:  # Use subset of validation for speed
            state = create_gps_reservoir(window, **params)
            pred = readout.predict(state)
            val_preds.append(pred)
        
        val_preds = np.array(val_preds)
        y_val_subset = y_val[:len(val_preds)]
        
        # Calculate BER - primary metric in research papers
        ber = calculate_ber(y_val_subset.flatten(), val_preds.flatten())
        
        # Calculate MSE as secondary metric
        mse = mean_squared_error(y_val_subset, val_preds)
        
        # Save result
        results.append({**params, 'ber': ber, 'mse': mse})
        
        # Update best params if better BER
        if ber < best_ber:
            best_ber = ber
            best_params = params
            print(f"New best BER: {best_ber:.6f} (MSE: {mse:.6f}) with params: {best_params}")
    
    # Save all results for analysis
    pd.DataFrame(results).to_csv('hyperparameter_optimization_results.csv', index=False)
    
    return best_params, best_ber

# Split data into training, validation, and testing sets
window_size = X_all.shape[1]  # Each window is a complete sample
n_train = int(0.7 * X_all.shape[0])  # Use 70% for training
n_val = int(0.15 * X_all.shape[0])   # Use 15% for validation

print(f"Using {n_train} windows for training, {n_val} for validation, {X_all.shape[0] - n_train - n_val} for testing")
print(f"Each window has {window_size} samples")

# Split the data
X_train_windows = X_all[:n_train]
y_train_windows = y_all[:n_train]
X_val_windows = X_all[n_train:n_train+n_val]
y_val_windows = y_all[n_train:n_train+n_val]
X_test_windows = X_all[n_train+n_val:]
y_test_windows = y_all[n_train+n_val:]

# Add jammer type information to preprocessing
print("Preprocessing signals with jammer-specific techniques...")
X_train_processed = []
X_val_processed = []
X_test_processed = []

# Create metadata subset to match the splits
metadata_train = metadata.iloc[:n_train]
metadata_val = metadata.iloc[n_train:n_train+n_val]
metadata_test = metadata.iloc[n_train+n_val:]

for i, window in enumerate(tqdm(X_train_windows, desc="Preprocessing training windows")):
    jammer_type = metadata_train.iloc[i]['jammer_type']
    X_train_processed.append(preprocess_signal(window, jammer_type=jammer_type))
    
for i, window in enumerate(tqdm(X_val_windows, desc="Preprocessing validation windows")):
    jammer_type = metadata_val.iloc[i]['jammer_type']
    X_val_processed.append(preprocess_signal(window, jammer_type=jammer_type))
    
for i, window in enumerate(tqdm(X_test_windows, desc="Preprocessing test windows")):
    jammer_type = metadata_test.iloc[i]['jammer_type']
    X_test_processed.append(preprocess_signal(window, jammer_type=jammer_type))

# Get best hyperparameters optimized for BER
best_params, best_ber = optimize_reservoir(X_train_processed, y_train_windows, 
                                         X_val_processed, y_val_windows,
                                         metadata_train, metadata_val, n_iter=15)

print(f"Best reservoir parameters found: {best_params} with BER: {best_ber:.6f}")

# Train models per jammer type for specialized performance
print("Training specialized models per jammer type...")
jammer_models = {}
jammer_types = metadata['jammer_type'].unique()

for jammer in jammer_types:
    print(f"\nTraining model for {jammer} jamming...")
    
    # Get indices for this jammer type
    train_idx = metadata_train['jammer_type'] == jammer
    
    if sum(train_idx) < 10:
        print(f"Not enough samples for {jammer}, skipping")
        continue
        
    # Create specialized states
    jammer_states = []
    for window in tqdm(np.array(X_train_processed)[train_idx], desc=f"Processing {jammer} windows"):
        state = create_gps_reservoir(window, **best_params)
        jammer_states.append(state)
    
    # Flatten for training
    X_jammer_flat = np.vstack(jammer_states)
    y_jammer_flat = y_train_windows[train_idx].reshape(-1)
    
    # Train model with optimal alpha
    readout = Ridge(alpha=0.05)
    readout.fit(X_jammer_flat, y_jammer_flat)
    
    # Store model
    jammer_models[jammer] = readout
    print(f"Trained model for {jammer} with {sum(train_idx)} windows")

# Create the reservoir states for each window in the training data
print("Creating reservoir states for training data...")
train_states = []
for window in tqdm(X_train_processed, desc="Processing training windows"):
    state = create_gps_reservoir(window, **best_params)
    train_states.append(state)

# Flatten all windows and states for training
X_train_flat = np.vstack(train_states)
y_train_flat = y_train_windows.reshape(-1)

# Try different alpha values for Ridge regression
alphas = [0.01, 0.1, 1.0, 10.0, 100.0]
best_alpha = 1.0
best_val_score = float('inf')

for alpha in alphas:
    readout = Ridge(alpha=alpha)
    readout.fit(X_train_flat, y_train_flat)
    
    # Quick validation on a subset
    val_preds = []
    for window in tqdm(X_val_processed[:50], desc=f"Validating alpha={alpha}"):
        state = create_gps_reservoir(window, **best_params)
        val_preds.append(readout.predict(state))
        
    val_score = mean_squared_error(y_val_windows[:50].reshape(len(val_preds), -1), val_preds)
    print(f"Alpha {alpha}: Validation MSE = {val_score:.6f}")
    
    if val_score < best_val_score:
        best_val_score = val_score
        best_alpha = alpha

print(f"Best Ridge alpha: {best_alpha}")

# Train final model with best alpha
readout = Ridge(alpha=best_alpha)
readout.fit(X_train_flat, y_train_flat)

# Test on unseen data
print("Testing model on unseen data...")
test_preds = []
for window in tqdm(X_test_processed, desc="Processing test windows"):
    state = create_gps_reservoir(window, **best_params)
    test_preds.append(readout.predict(state))

# Convert predictions to array
test_preds = np.array(test_preds)

# Evaluate with multiple metrics
metrics = evaluate_model(y_test_windows, test_preds)
print("\nModel Performance Metrics:")
for metric, value in metrics.items():
    print(f"{metric}: {value:.6f}")

# Visualize results for a few test windows
plt.figure(figsize=(15, 10))

# Show multiple test cases
num_examples = min(3, len(test_preds))
for i in range(num_examples):
    plt.subplot(num_examples, 3, i*3 + 1)
    plt.plot(X_test_windows[i], label='Jammed Signal (Input)')
    plt.title(f'Example {i+1}: Jammed Signal')
    plt.legend()
    
    plt.subplot(num_examples, 3, i*3 + 2)
    plt.plot(test_preds[i], label='Predicted Clean Signal')
    plt.plot(y_test_windows[i], 'r--', label='True Clean Signal')
    plt.title('Clean Signal Prediction vs Ground Truth')
    plt.legend()
    
    plt.subplot(num_examples, 3, i*3 + 3)
    plt.plot(y_test_windows[i] - test_preds[i], label='Error')
    plt.title('Error (True - Predicted)')
    plt.legend()

plt.tight_layout()
plt.savefig('reservoir_computing_results.png')
plt.show()

# Save the trained model with best parameters
with open('rc_model_optimized.pkl', 'wb') as f:
    pickle.dump({
        'readout': readout, 
        'reservoir_params': best_params,
        'metrics': metrics
    }, f)
print("Optimized model saved as 'rc_model_optimized.pkl'")

# Also add jammer type-specific analysis
jammer_analysis = {}
jammer_types = metadata['jammer_type'].unique()

for jammer_type in jammer_types:
    # Get test indices for this jammer type
    test_meta = metadata.iloc[n_train+n_val:]
    jammer_indices = test_meta[test_meta['jammer_type'] == jammer_type].index.values - (n_train+n_val)
    
    if len(jammer_indices) == 0:
        continue
        
    # Get predictions and true values for this jammer type
    jammer_preds = test_preds[jammer_indices]
    jammer_true = y_test_windows[jammer_indices]
    
    # Calculate metrics
    jammer_metrics = evaluate_model(jammer_true, jammer_preds)
    jammer_analysis[jammer_type] = jammer_metrics
    
    print(f"\nPerformance on {jammer_type} jammers:")
    for metric, value in jammer_metrics.items():
        print(f"{metric}: {value:.6f}")

# Add jammer-specific visualization
plt.figure(figsize=(15, 10))
plt.title("MSE by Jammer Type")
jammer_mses = [metrics['mse'] for jammer_type, metrics in jammer_analysis.items()]
plt.bar(jammer_types, jammer_mses)
plt.xlabel("Jammer Type")
plt.ylabel("Mean Squared Error")
plt.tight_layout()
plt.savefig('jammer_type_performance.png')

# Create a more sophisticated ensemble approach for better predictions
print("Training ensemble of reservoirs for better oscillation capture...")
# Create multiple reservoirs with different parameters to capture different aspects
n_ensemble = 3  # Use 3 different reservoirs

ensemble_params = [
    # First reservoir: good for general signal structure
    {**best_params, 'leaking_rate': max(0.1, best_params['leaking_rate'] * 0.8)},
    
    # Second reservoir: better for fast oscillations
    {**best_params, 'spectral_radius': min(0.99, best_params['spectral_radius'] * 1.1),
     'leaking_rate': min(0.9, best_params['leaking_rate'] * 1.5)},
     
    # Third reservoir: specialized for peaks
    {**best_params, 'input_scaling': best_params['input_scaling'] * 1.2,
     'n_reservoir': int(best_params['n_reservoir'] * 1.2)}
]

# Train ensemble
ensemble_readouts = []
for i, params in enumerate(ensemble_params):
    print(f"Training ensemble member {i+1}/{n_ensemble}...")
    # Create reservoir states
    train_states = []
    for window in tqdm(X_train_processed, desc=f"Processing windows for ensemble {i+1}"):
        state = create_gps_reservoir(window, **params)
        train_states.append(state)
    
    # Flatten and train
    X_train_flat = np.vstack(train_states)
    readout = Ridge(alpha=best_alpha)
    readout.fit(X_train_flat, y_train_flat)
    ensemble_readouts.append((readout, params))

# Test ensemble
print("Testing ensemble model...")
ensemble_preds = []

for window in tqdm(X_test_processed, desc="Processing test windows"):
    # Get predictions from each ensemble member
    member_preds = []
    for readout, params in ensemble_readouts:
        state = create_gps_reservoir(window, **params)
        pred = readout.predict(state)
        member_preds.append(pred)
    
    # Combine predictions with weighted average
    # Give more weight to the model that better captures peaks
    ensemble_pred = 0.3 * member_preds[0] + 0.3 * member_preds[1] + 0.4 * member_preds[2]
    ensemble_preds.append(ensemble_pred)

# Convert to array
ensemble_preds = np.array(ensemble_preds)

# Evaluate ensemble performance
ensemble_metrics = evaluate_model(y_test_windows, ensemble_preds)
print("\nEnsemble Model Performance Metrics:")
for metric, value in ensemble_metrics.items():
    print(f"{metric}: {value:.6f}")

# Compare single model vs ensemble
plt.figure(figsize=(15, 10))
i = 0  # Show first test example
plt.subplot(311)
plt.plot(X_test_windows[i], label='Jammed Signal', alpha=0.7)
plt.title('Input Jammed Signal')
plt.legend()

plt.subplot(312)
plt.plot(y_test_windows[i], 'g-', label='True Clean Signal', linewidth=2)
plt.plot(test_preds[i], 'b--', label='Single RC Prediction')
plt.plot(ensemble_preds[i], 'r-', label='Ensemble RC Prediction', linewidth=1.5)
plt.title('True vs Predicted Signals')
plt.legend()

plt.subplot(313)
plt.plot(y_test_windows[i] - test_preds[i], 'b--', label='Single RC Error')
plt.plot(y_test_windows[i] - ensemble_preds[i], 'r-', label='Ensemble RC Error')
plt.title('Prediction Errors (Lower is Better)')
plt.legend()

plt.tight_layout()
plt.savefig('rc_model_comparison.png', dpi=300)

# Add frequency domain analysis
plt.figure(figsize=(15, 8))
i = 0  # Show same example in frequency domain
from scipy import fft

# Calculate FFTs
true_fft = np.abs(fft.fft(y_test_windows[i]))[:len(y_test_windows[i])//2]
single_fft = np.abs(fft.fft(test_preds[i]))[:len(test_preds[i])//2]
ensemble_fft = np.abs(fft.fft(ensemble_preds[i]))[:len(ensemble_preds[i])//2]
freq = fft.fftfreq(len(y_test_windows[i]), 1/1000)[:len(y_test_windows[i])//2]

plt.semilogy(freq, true_fft, 'g-', label='True Signal')
plt.semilogy(freq, single_fft, 'b--', label='Single RC')
plt.semilogy(freq, ensemble_fft, 'r-', label='Ensemble RC')
plt.title('Frequency Domain Analysis')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Magnitude')
plt.legend()
plt.grid(True)
plt.savefig('frequency_domain_comparison.png', dpi=300)

# Save the ensemble model
with open('rc_ensemble_model.pkl', 'wb') as f:
    pickle.dump({
        'ensemble_members': ensemble_readouts,
        'metrics': ensemble_metrics,
        'preprocessing': 'signal_with_derivatives'
    }, f)
print("Ensemble model saved as 'rc_ensemble_model.pkl'")

# Updated prediction function for ensemble model
def predict_clean_signal_ensemble(jammed_signal, model_file='rc_ensemble_model.pkl'):
    """Use the ensemble model to predict a clean signal from a jammed one"""
    # Load the model
    with open(model_file, 'rb') as f:
        model_data = pickle.load(f)
    
    ensemble_members = model_data['ensemble_members']
    
    # Preprocess input
    processed_signal = preprocess_signal(jammed_signal)
    
    # Get predictions from each ensemble member
    member_preds = []
    for readout, params in ensemble_members:
        state = create_gps_reservoir(processed_signal, **params)
        pred = readout.predict(state)
        member_preds.append(pred)
    
    # Combine predictions with weighted average
    ensemble_pred = 0.3 * member_preds[0] + 0.3 * member_preds[1] + 0.4 * member_preds[2]
    
    return ensemble_pred

print("\nTo make predictions on new jammed signals, use:")
print("clean_signal = predict_clean_signal_ensemble(jammed_signal)")

# Add a completely new approach function - try ESN with Ridge and LASSO ensemble
def train_advanced_esn_ensemble(X_train, y_train, best_params):
    """Create advanced ensemble with Ridge and Lasso to better capture peaks"""
    from sklearn.linear_model import Ridge, Lasso, ElasticNet
    
    print("Training advanced ensemble for peak and oscillation detection...")
    
    # Create reservoir states with best parameters
    train_states = []
    for window in tqdm(X_train, desc="Processing training windows"):
        state = create_gps_reservoir(window, **best_params)
        train_states.append(state)
    
    # Flatten states for training
    X_states = np.vstack(train_states)
    y_flat = y_train.reshape(-1)
    
    # Train multiple models with different regularization approaches
    # 1. Ridge for general pattern learning
    ridge = Ridge(alpha=0.1)
    ridge.fit(X_states, y_flat)
    
    # 2. Lasso for sparse feature selection (may help capture peaks better)
    lasso = Lasso(alpha=0.001, max_iter=10000)
    lasso.fit(X_states, y_flat)
    
    # 3. ElasticNet combines both L1 and L2 penalties
    elastic = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000)
    elastic.fit(X_states, y_flat)
    
    # Return ensemble
    return {
        'ridge': ridge,
        'lasso': lasso,
        'elastic': elastic,
        'params': best_params
    }

# Predict with the advanced ensemble
def predict_with_advanced_ensemble(X_test, ensemble):
    """Make predictions using the advanced ensemble"""
    test_preds = []
    
    for window in tqdm(X_test, desc="Predicting with advanced ensemble"):
        # Create reservoir states
        states = create_gps_reservoir(window, **ensemble['params'])
        
        # Get predictions from each model
        ridge_pred = ensemble['ridge'].predict(states)
        lasso_pred = ensemble['lasso'].predict(states)
        elastic_pred = ensemble['elastic'].predict(states)
        
        # Weight the predictions - give more weight to the models that capture peaks better
        # Lasso tends to be better at selecting features related to peaks
        ensemble_pred = 0.3 * ridge_pred + 0.4 * lasso_pred + 0.3 * elastic_pred
        test_preds.append(ensemble_pred)
    
    return np.array(test_preds)

# After finding best parameters, train advanced ensemble
advanced_ensemble = train_advanced_esn_ensemble(X_train_processed, y_train_windows, best_params)

# Predict with advanced ensemble
advanced_predictions = predict_with_advanced_ensemble(X_test_processed, advanced_ensemble)

# Evaluate advanced ensemble
advanced_metrics = evaluate_model(y_test_windows, advanced_predictions)
print("\nAdvanced Ensemble Performance Metrics:")
for metric, value in advanced_metrics.items():
    print(f"{metric}: {value:.6f}")

# Compare all approaches
plt.figure(figsize=(16, 12))
i = 0  # Show first test example

# Sample indices where peaks occur in the true signal
peak_indices = sg.find_peaks(np.abs(y_test_windows[i]), height=0.2)[0]
if len(peak_indices) == 0:  # If no clear peaks, just choose a representative section
    peak_indices = [len(y_test_windows[i])//2]

# Plot around a peak for better visualization
idx = peak_indices[0]
window_size = 50  # Show 50 samples around the peak
start_idx = max(0, idx - window_size//2)
end_idx = min(len(y_test_windows[i]), idx + window_size//2)

plt.subplot(411)
plt.plot(y_test_windows[i][start_idx:end_idx], 'g-', label='True Clean Signal', linewidth=2)
plt.axvline(idx-start_idx, color='k', linestyle='--', alpha=0.3)
plt.title('Ground Truth Signal (zoomed to peak)')
plt.legend()

plt.subplot(412)
plt.plot(y_test_windows[i][start_idx:end_idx], 'g-', label='True Signal', linewidth=2, alpha=0.7)
plt.plot(test_preds[i][start_idx:end_idx], 'b-', label='Basic RC')
plt.axvline(idx-start_idx, color='k', linestyle='--', alpha=0.3)
plt.title('Basic RC Prediction')
plt.legend()

plt.subplot(413)
plt.plot(y_test_windows[i][start_idx:end_idx], 'g-', label='True Signal', linewidth=2, alpha=0.7)
plt.plot(ensemble_preds[i][start_idx:end_idx], 'm-', label='Ensemble RC')
plt.axvline(idx-start_idx, color='k', linestyle='--', alpha=0.3)
plt.title('Ensemble RC Prediction')
plt.legend()

plt.subplot(414)
plt.plot(y_test_windows[i][start_idx:end_idx], 'g-', label='True Signal', linewidth=2, alpha=0.7)
plt.plot(advanced_predictions[i][start_idx:end_idx], 'r-', label='Advanced RC')
plt.axvline(idx-start_idx, color='k', linestyle='--', alpha=0.3)
plt.title('Advanced RC Prediction (with wavelet preprocessing)')
plt.legend()

plt.tight_layout()
plt.savefig('peak_detection_comparison.png', dpi=300)

# Save the advanced model
with open('rc_advanced_model.pkl', 'wb') as f:
    pickle.dump({
        'ensemble': advanced_ensemble,
        'metrics': advanced_metrics,
        'preprocessing': 'wavelet_hilbert_ensemble'
    }, f)
print("Advanced model saved as 'rc_advanced_model.pkl'")

# Updated prediction function for advanced model
def predict_clean_signal_advanced(jammed_signal, model_file='rc_advanced_model.pkl'):
    """Use the advanced model to predict a clean signal from a jammed one"""
    # Load the model
    with open(model_file, 'rb') as f:
        model_data = pickle.load(f)
    
    ensemble = model_data['ensemble']
    
    # Preprocess input
    processed_signal = preprocess_signal(jammed_signal)
    
    # Create reservoir states
    states = create_gps_reservoir(processed_signal, **ensemble['params'])
    
    # Get predictions from each model
    ridge_pred = ensemble['ridge'].predict(states)
    lasso_pred = ensemble['lasso'].predict(states)
    elastic_pred = ensemble['elastic'].predict(states)
    
    # Weighted ensemble
    ensemble_pred = 0.3 * ridge_pred + 0.4 * lasso_pred + 0.3 * elastic_pred
    
    return ensemble_pred

print("\nTo make predictions with the advanced model, use:")
print("clean_signal = predict_clean_signal_advanced(jammed_signal)")